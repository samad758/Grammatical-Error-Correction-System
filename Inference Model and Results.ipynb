{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ff62f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 10:12:28.121490: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 10:12:29.051532: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/josephnadar1998/miniconda3/envs/tf/lib/\n",
      "2023-03-15 10:12:29.051662: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/josephnadar1998/miniconda3/envs/tf/lib/\n",
      "2023-03-15 10:12:29.051674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import seaborn as sns\n",
    "import re\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, TFBertForMaskedLM,TFDistilBertForMaskedLM\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8253585",
   "metadata": {},
   "source": [
    "### Importing all the pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c354a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[vocab_size_correct,vocab_size_incorrect,correct_tk,incorrect_tk]=pickle.load(open('tokenizer_files.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755008d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_m_1,test_m_1, validation_m_1]=pickle.load(open('main_data_2.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898dd59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_m_2,test_m_2, validation_m_2]=pickle.load(open('main_data_2_reverse.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993db26c",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cf5b2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.embedding_layer=Embedding(input_dim=inp_vocab_size,output_dim=embedding_size,input_length=input_length, mask_zero=True )\n",
    "        self.lstm_layer=LSTM(lstm_size, return_sequences=True, return_state=True)\n",
    "        self.lstm_size=lstm_size\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        '''\n",
    "        This function takes a sequence input and the initial states of the encoder.\n",
    "        Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "        returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "        '''\n",
    "        input_1=self.embedding_layer(input_sequence)\n",
    "        output, output_h, output_c=self.lstm_layer(input_1, initial_state=states)\n",
    "        return output, output_h, output_c\n",
    "\n",
    "    def initialize_states(self,batch_size):\n",
    "        '''\n",
    "        Given a batch size it will return intial hidden state and intial cell state.\n",
    "        If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "        '''\n",
    "        output_h, output_c=tf.zeros([batch_size,self.lstm_size]), tf.zeros([batch_size,self.lstm_size])\n",
    "        return output_h, output_c\n",
    "\n",
    "#Attention#\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "    '''\n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        super().__init__()\n",
    "        # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "        \n",
    "        self.att_units=att_units\n",
    "        self.scoring_function=scoring_function\n",
    "        self.dot=tf.keras.layers.Dot(axes=(1,2))\n",
    "        self.mult=tf.keras.layers.Multiply()\n",
    "        self.add=tf.keras.layers.Add()\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def call(self,decoder_hidden_state,encoder_output):\n",
    "      '''\n",
    "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "      Multiply the score function with your encoder_outputs to get the context vector.\n",
    "      Function returns context vector and attention weights(softmax - scores)\n",
    "      '''\n",
    "      # Implement Dot score function here\n",
    "      #print('decoder_hidden_state',tf.expand_dims(decoder_hidden_state,1).shape, 'encoder_output', encoder_output.shape)\n",
    "      alphas=tf.matmul(encoder_output,tf.expand_dims(decoder_hidden_state,-1))\n",
    "      alphas=tf.nn.softmax(alphas)\n",
    "      context_vector=alphas*encoder_output\n",
    "      context_vector=tf.reduce_sum(context_vector, axis=1)\n",
    "      return context_vector,alphas\n",
    "    \n",
    "class One_Step_Decoder(tf.keras.Model):\n",
    "    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super().__init__()\n",
    "        # Initialize decoder embedding layer, LSTM and any other objects needed #, mask_zero=True, trainable=True ,weights=[embedding_matrix]\n",
    "        self.embedding_layer=Embedding(input_dim=tar_vocab_size, output_dim=embedding_dim, input_length=input_length, mask_zero=True, trainable=True )\n",
    "        self.lstm_layer=LSTM(dec_units, return_state=True, return_sequences=True)\n",
    "        self.att_units=att_units\n",
    "        self.score_fun=score_fun\n",
    "        self.tar_vocab_size=tar_vocab_size\n",
    "        self.dec_units=dec_units\n",
    "        self.dense_layer=tf.keras.layers.Dense(tar_vocab_size)\n",
    "        self.attention=Attention(score_fun,att_units)\n",
    "\n",
    "\n",
    "    def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "        '''\n",
    "        One step decoder mechanisim step by step:\n",
    "          A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "          B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "          C. Concat the context vector with the step A output\n",
    "          D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "          E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "          F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "        '''\n",
    "        result=self.embedding_layer(input_to_decoder)\n",
    "        result=tf.squeeze(result, axis=1)\n",
    "\n",
    "        context_vector, weights=self.attention(state_h, encoder_output)\n",
    "\n",
    "        output_1=tf.concat([context_vector, result],axis=1)\n",
    "        output_1=tf.expand_dims(output_1,1)\n",
    "        \n",
    "        decoder_outputs, decoder_h, decoder_c=self.lstm_layer(output_1, initial_state=[state_h,state_c])\n",
    "\n",
    "        \n",
    "        final_output=self.dense_layer(decoder_outputs)\n",
    "        final_output=tf.squeeze(final_output,axis=1)\n",
    "        \n",
    "        return final_output,decoder_h, decoder_c, weights,context_vector\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "      super().__init__()\n",
    "\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "      \n",
    "      self.input_length=input_length\n",
    "      self.dec_units=dec_units\n",
    "      self.score_fun=score_fun\n",
    "      self.att_units=att_units\n",
    "      self.out_vocab_size=out_vocab_size\n",
    "      self.embedding_dim=embedding_dim\n",
    "      self.osd=One_Step_Decoder(tar_vocab_size=self.out_vocab_size, embedding_dim=self.embedding_dim, \n",
    "                                                                                   input_length=self.input_length, dec_units=self.dec_units, score_fun=self.score_fun, att_units=self.att_units)\n",
    "      pass\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    @tf.function\n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
    "\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "        \n",
    "        #Iterate till the length of the decoder input\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            # Store the output in tensorarray\n",
    "        # Return the tensor array\n",
    "        #print(input_to_decoder.shape)\n",
    "        \n",
    "        output_array=tf.TensorArray(tf.float32,size=input_to_decoder.shape[1])\n",
    "        #print('input_to_decoder',input_to_decoder.shape)\n",
    "        for timestep in range(input_to_decoder.shape[1]):\n",
    "          #print(input_to_decoder.shape, encoder_output.shape, decoder_hidden_state.shape,decoder_cell_state.shape)\n",
    "          output,decoder_hidden_state,decoder_cell_state,attention_weights,context_vector=self.osd(input_to_decoder[:,timestep:timestep+1], encoder_output, decoder_hidden_state,decoder_cell_state)\n",
    "          output_array = output_array.write(timestep, output)\n",
    "          #output_array.write(timestep,output).mark_used()\n",
    "        #.mark_used()\n",
    "        all_output=tf.transpose(output_array.stack(), [1,0,2])\n",
    "        #print(all_output.shape)\n",
    "        return all_output\n",
    "\n",
    "class encoder_decoder(tf.keras.Model):\n",
    "  def __init__(self,inp_vocab_size,out_vocab_size, embedding_size, lstm_size, input_length_l1, input_length_l2, dec_units, score_fun, att_units, batch_size):\n",
    "    super().__init__()\n",
    "    #Intialize objects from encoder decoder\n",
    "    self.encoder_block=Encoder(inp_vocab_size=inp_vocab_size,embedding_size=embedding_size, lstm_size=lstm_size, input_length=input_length_l1 )\n",
    "    self.decoder_block=Decoder(out_vocab_size=out_vocab_size, embedding_dim=embedding_size, input_length=input_length_l2, dec_units=dec_units, score_fun=score_fun, att_units=att_units)\n",
    "    self.batch_size=batch_size\n",
    "    pass\n",
    "\n",
    "  \n",
    "  def call(self,data):\n",
    "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "    # return the decoder output\n",
    "    input_sequence=data[0]\n",
    "    output_sequence=data[1]\n",
    "    #print(input_sequence.shape)\n",
    "    encoder_h, encoder_c=self.encoder_block.initialize_states(self.batch_size)\n",
    "    encoder_output, encoder_h, encoder_c=self.encoder_block(input_sequence, states=[encoder_h, encoder_c])\n",
    "    #input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state\n",
    "    dec_h,dec_c=encoder_h, encoder_c\n",
    "    output_decoder =self.decoder_block(input_to_decoder=output_sequence,encoder_output=encoder_output,decoder_hidden_state=dec_h,decoder_cell_state=dec_c)\n",
    "    #output_decoder=self.soft_max(output_decoder)\n",
    "\n",
    "    return output_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a75e60",
   "metadata": {},
   "source": [
    "This model takes input and output in a normal manner(Left to Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb279530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 10:12:36.042024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-15 10:12:36.186131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-15 10:12:36.186976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-15 10:12:36.191682: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 10:12:36.193972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-15 10:12:36.194712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-15 10:12:36.195346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-15 10:12:38.711546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-15 10:12:38.714288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-15 10:12:38.714956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-15 10:12:38.718268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14620 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "input_vocab_size= vocab_size_incorrect+1\n",
    "output_vocab_size=vocab_size_correct+1\n",
    "embedding_size=300\n",
    "lstm_size=512\n",
    "input_len=16\n",
    "output_len=16\n",
    "dec_units=512\n",
    "score_fun='dot'\n",
    "att_units=512\n",
    "BATCH_SIZE=512\n",
    "\n",
    "model_1 = encoder_decoder(input_vocab_size,output_vocab_size,embedding_size,lstm_size,input_len,output_len,dec_units,score_fun,att_units, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af95e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.build((None,512,16))\n",
    "model_1.load_weights('model_2/model_2_epoch_17.h5')\n",
    "#left to right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db053e37",
   "metadata": {},
   "source": [
    "This model takes input in reverse and output in a normal manner(Left to Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c2fa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = encoder_decoder(input_vocab_size,output_vocab_size,embedding_size,lstm_size,input_len,output_len,dec_units,score_fun,att_units, BATCH_SIZE)\n",
    "model_2.build((None,512,16))\n",
    "model_2.load_weights('model_3/model_3_epoch_46.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86a0b9",
   "metadata": {},
   "source": [
    "### Language Model to Check the Fluency Score\n",
    "- In the paper, the author have suggested to use a language model to get a fluency score of a sentence at each level to decide whether a sentence needs to be trained again and other aspects.\n",
    "- In the paper they have used a 5-gram model, but here I have chosen to use a BERT(Transformer) based model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1e512",
   "metadata": {},
   "source": [
    "### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c92322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def p_x(word, sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"tf\")\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "    #The word I am expecting\n",
    "\n",
    "    mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
    "    selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "    selected_logits=tf.math.sigmoid(selected_logits)\n",
    "\n",
    "    w=tokenizer.encode(word)[1]\n",
    "    return selected_logits[0][w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c92d66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluency_score(sent):\n",
    "    words=sent.split(' ')\n",
    "    l=len(words)\n",
    "    scores=[]\n",
    "    context=['[MASK]']\n",
    "    i=0\n",
    "    for w in words:\n",
    "        if i==0:\n",
    "            #print(w)\n",
    "            p=p_x(w,'[MASK]')\n",
    "            context.insert(0,w)\n",
    "        else:\n",
    "            #print(w,' '.join(context))\n",
    "            p=p_x(w,' '.join(context))\n",
    "        i+=1\n",
    "        context.insert(0,w)\n",
    "        #print(p)\n",
    "        scores.append(p)\n",
    "    h_x=-(sum(scores)/l)\n",
    "    #print(h_x)\n",
    "    #print(scores)\n",
    "    f_score=1/(1+h_x)\n",
    "    return f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ce56b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluency Score is tf.Tensor(2.651366, shape=(), dtype=float32)\n",
      "0.8333444595336914\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i=time.time()\n",
    "print('Fluency Score is',fluency_score('Today is a great day'))\n",
    "print(time.time()-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c05db85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluency Score is tf.Tensor(1.7072877, shape=(), dtype=float32)\n",
      "0.49550795555114746\n"
     ]
    }
   ],
   "source": [
    "i=time.time()\n",
    "print('Fluency Score is',fluency_score('Today great day'))\n",
    "print(time.time()-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dfca4b",
   "metadata": {},
   "source": [
    "### Using more faster transformers\n",
    "- The BERT is taking more time, and hence using DistilBERT which has less parameter and hence is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "998d3380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = TFDistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def p_x(word, sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"tf\")\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "    #The word I am expecting\n",
    "\n",
    "    mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n",
    "    selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "    selected_logits=tf.math.sigmoid(selected_logits)\n",
    "\n",
    "    w=tokenizer.encode(word)[1]\n",
    "    return selected_logits[0][w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "27b0727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42206382751464844\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i=time.time()\n",
    "fluency_score('Today is a great day')\n",
    "print(time.time()-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cc721bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluency Score is tf.Tensor(2.579536, shape=(), dtype=float32)\n",
      "0.269848108291626\n"
     ]
    }
   ],
   "source": [
    "i=time.time()\n",
    "print('Fluency Score is',fluency_score('Today great day'))\n",
    "print(time.time()-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c3a67",
   "metadata": {},
   "source": [
    "**DistilBERT takes half the time of BERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf5596",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- Here we can clearly see, that a poor sentence gets a low fluency score, whereas a good sentence gets a high fluency score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baeb211",
   "metadata": {},
   "source": [
    "### Checking Individual Performance for each Model alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0bb019",
   "metadata": {},
   "source": [
    "# 1. Left to Right Model\n",
    "- Where the sentence are feeded from left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "174c69fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she like dance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'she likes to dance'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_m1(input_sentence):\n",
    "\n",
    "    words=[]\n",
    "    input_sentence=[input_sentence]\n",
    "    batch_size=1\n",
    "    tokenized_sent=incorrect_tk.texts_to_sequences(input_sentence)\n",
    "    #print(tokenized_sent)\n",
    "    padded_sent=tf.keras.utils.pad_sequences(tokenized_sent, maxlen=16,padding='post' )\n",
    "    encoder_h, encoder_c=model_1.layers[0].initialize_states(batch_size)\n",
    "    encoder_output,encoder_h, encoder_c= model_1.layers[0](padded_sent, states=[encoder_h, encoder_c])\n",
    "\n",
    "    start_index=correct_tk.word_index.get('<start>')\n",
    "    end_index=correct_tk.word_index.get('<end>')\n",
    "    for i in range(20):\n",
    "        decoder_output, decoder_h, decoder_c, attention_weights, context_vector = model_1.layers[1].osd(tf.convert_to_tensor([[start_index]]), encoder_output, encoder_h, encoder_c)\n",
    "     \n",
    "        output_index=np.argmax(decoder_output[0])\n",
    "        start_index=output_index\n",
    "        #print(output_index)\n",
    "        encoder_h, encoder_c=decoder_h, decoder_c\n",
    "        \n",
    "        if output_index==end_index:\n",
    "            break;\n",
    "            \n",
    "        if i==0:\n",
    "            words.append(correct_tk.index_word[output_index])\n",
    "        else:\n",
    "            if words[-1]!=correct_tk.index_word[output_index]:\n",
    "                words.append(correct_tk.index_word[output_index])\n",
    "\n",
    "        #print(list(tknizer_eng.word_index.keys())[output_index])\n",
    "\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "i='she like dance'\n",
    "print(i)\n",
    "predict_m1(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "43403990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 5564.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#Results on using Capital Sentences and fullstops\n",
    "samples=test_m_1['incorrect'].sample(1000)\n",
    "predicted_samples_model_2=samples.apply(predict_m1)\n",
    "\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "blue_scores=[]\n",
    "\n",
    "for i in tqdm(range(len(samples))):\n",
    "  reference=samples.values[i]\n",
    "  translation=predicted_samples_model_2.values[i]\n",
    "  b=bleu.sentence_bleu([reference], translation)\n",
    "  blue_scores.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "577163c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Blue Score for the 1000 Samples is 66.03647240981525 %\n"
     ]
    }
   ],
   "source": [
    "print('The Blue Score for the 1000 Samples is',np.mean(np.array(blue_scores))*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dff9ffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500/500 [00:42<00:00, 11.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37295990225914544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the Glue Score\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "def calculate_glue_on_df(df, predict):\n",
    "    glue_score_arr = []\n",
    "    for i in tqdm(range(500)):\n",
    "        reference = [df['correct'].iloc[i].split()]\n",
    "        pred = predict(df['incorrect'].iloc[i])\n",
    "        candidate = pred.split()\n",
    "        glue_score_arr.append(sentence_gleu(reference, candidate))\n",
    "    return np.mean(glue_score_arr)\n",
    "\n",
    "def gleu(sentence):\n",
    "  pred = predict(sentence)[0]\n",
    "  return sentence_gleu([sentence.split()], pred.split())\n",
    "\n",
    "        \n",
    "glue_m1=calculate_glue_on_df(test_m_1, predict_m1)\n",
    "print(glue_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "36481c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=0.37295990225914544"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6687e",
   "metadata": {},
   "source": [
    "# 2. Right to Left Model\n",
    "- Model in which sentences are fed in a reverse fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9e18a260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main reason is probably is because i just want to life freely\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the main reason is probably because i have always liked to life freely'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_m2(input_sentence):\n",
    "    \n",
    "    input_sentence=' '.join(input_sentence.split(' ')[::-1])\n",
    "    words=[]\n",
    "    input_sentence=[input_sentence]\n",
    "    batch_size=1\n",
    "    tokenized_sent=incorrect_tk.texts_to_sequences(input_sentence)\n",
    "    #print(tokenized_sent)\n",
    "    padded_sent=tf.keras.utils.pad_sequences(tokenized_sent, maxlen=16,padding='post' )\n",
    "    encoder_h, encoder_c=model_2.layers[0].initialize_states(batch_size)\n",
    "    encoder_output,encoder_h, encoder_c= model_2.layers[0](padded_sent, states=[encoder_h, encoder_c])\n",
    "\n",
    "    start_index=correct_tk.word_index.get('<start>')\n",
    "    end_index=correct_tk.word_index.get('<end>')\n",
    "    for i in range(20):\n",
    "        decoder_output, decoder_h, decoder_c, attention_weights, context_vector = model_2.layers[1].osd(tf.convert_to_tensor([[start_index]]), encoder_output, encoder_h, encoder_c)\n",
    "     \n",
    "        output_index=np.argmax(decoder_output[0])\n",
    "        start_index=output_index\n",
    "        #print(output_index)\n",
    "        encoder_h, encoder_c=decoder_h, decoder_c\n",
    "        if output_index==end_index:\n",
    "            break;\n",
    "            \n",
    "        if i==0:\n",
    "            words.append(correct_tk.index_word[output_index])\n",
    "        else:\n",
    "            if words[-1]!=correct_tk.index_word[output_index]:\n",
    "                words.append(correct_tk.index_word[output_index])\n",
    "\n",
    "        #print(list(tknizer_eng.word_index.keys())[output_index])\n",
    "\n",
    "        \n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "i=test_m_2.iloc[10]['incorrect']\n",
    "print(i)\n",
    "predict_m2(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfc57147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 0/1000 [00:00<?, ?it/s]/home/josephnadar1998/miniconda3/envs/tf/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      " 56%|██████████████████████████████████████▊                              | 563/1000 [00:00<00:00, 5625.36it/s]/home/josephnadar1998/miniconda3/envs/tf/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 5573.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#Results on using Capital Sentences and fullstops\n",
    "samples=test_m_2['incorrect'].sample(1000)\n",
    "predicted_samples_model_2=samples.apply(predict_m2)\n",
    "\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "blue_scores=[]\n",
    "\n",
    "for i in tqdm(range(len(samples))):\n",
    "  reference=samples.values[i]\n",
    "  translation=predicted_samples_model_2.values[i]\n",
    "  b=bleu.sentence_bleu([reference], translation)\n",
    "  blue_scores.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7819bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Blue Score for the 1000 Samples is 67.58336295010623 %\n"
     ]
    }
   ],
   "source": [
    "print('The Blue Score for the 1000 Samples is',np.mean(np.array(blue_scores))*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "761850cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500/500 [00:43<00:00, 11.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3876501519122926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the Glue Score\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "def calculate_glue_on_df(df, predict):\n",
    "    glue_score_arr = []\n",
    "    for i in tqdm(range(500)):\n",
    "        reference = [df['correct'].iloc[i].split()]\n",
    "        pred = predict(df['incorrect'].iloc[i])\n",
    "        candidate = pred.split()\n",
    "        glue_score_arr.append(sentence_gleu(reference, candidate))\n",
    "    return np.mean(glue_score_arr)\n",
    "\n",
    "def gleu(sentence):\n",
    "  pred = predict(sentence)[0]\n",
    "  return sentence_gleu([sentence.split()], pred.split())\n",
    "\n",
    "        \n",
    "glue_m2=calculate_glue_on_df(test_m_2, predict_m2)\n",
    "print(glue_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "289a928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2=0.3876501519122926\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9673b98",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "- Individually the model 2 in which sentences were fed in a reverse manner is performing better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122aa01",
   "metadata": {},
   "source": [
    "### Using both the models for Inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6487684",
   "metadata": {},
   "source": [
    "## 1. As mentioned in the paper in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5b2ce1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i started working from the end week of july\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i started working last week of july'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference_1(sentence):\n",
    "    #passing the sentence through model_1\n",
    "    #passing the output to model_2\n",
    "    for i in range(2):\n",
    "        predicted_sentence_1=predict_m1(sentence)\n",
    "        f1=fluency_score(predicted_sentence_1)\n",
    "        predicted_sentence_2=predict_m2(predicted_sentence_1)\n",
    "        \n",
    "        f2=fluency_score(predicted_sentence_2)\n",
    "        sentence=predicted_sentence_2\n",
    "        if f1==f2:\n",
    "            break;\n",
    "    return sentence\n",
    "    \n",
    "j=test_m_2.iloc[120]['incorrect']\n",
    "print(j)\n",
    "inference_1(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "31e41d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500/500 [25:48<00:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2667352870663321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the Glue Score\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "def calculate_glue_on_df(df, predict):\n",
    "    glue_score_arr = []\n",
    "    for i in tqdm(range(500)):\n",
    "        reference = [df['correct'].iloc[i].split()]\n",
    "        pred = predict(df['incorrect'].iloc[i])\n",
    "        candidate = pred.split()\n",
    "        glue_score_arr.append(sentence_gleu(reference, candidate))\n",
    "    return np.mean(glue_score_arr)\n",
    "\n",
    "def gleu(sentence):\n",
    "  pred = predict(sentence)[0]\n",
    "  return sentence_gleu([sentence.split()], pred.split())\n",
    "\n",
    "        \n",
    "glue_m2=calculate_glue_on_df(test_m_2, inference_1)\n",
    "print(glue_m2)\n",
    "\n",
    "#http://nlpprogress.com/english/grammatical_error_correction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "03b3ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper=0.2667352870663321"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2be705",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- Here the GLUE Score is less for this method of inference, however it is performing good for some sentences which I will be discussing in the last."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87097f28",
   "metadata": {},
   "source": [
    "Here I was trying to understand which metric best captures our GEC problem and could see that Fluency Score captures it better, but the performance of a model cannot be fully gauged based on this.\n",
    "\n",
    "Also GLUE Score, of greater than 30 is considered to be a good model, because there are multiple ways to speak the same sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1fd75bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i started working from the end week of july\n",
      "i started working at the end of july\n",
      "i started working at the end of july\n"
     ]
    }
   ],
   "source": [
    "def inference(sentence):\n",
    "    predicted_sentence_1=predict_m2(sentence)\n",
    "    print(predicted_sentence_1)\n",
    "    predicted_sentence_2=predict_m1(predicted_sentence_1)\n",
    "    print(predicted_sentence_2)\n",
    "    return predicted_sentence_1,predicted_sentence_2\n",
    "i=test_m_2.iloc[120]['incorrect']\n",
    "print(i)\n",
    "a,b=inference(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "96b88eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.839783, shape=(), dtype=float32)\n",
      "tf.Tensor(1.839783, shape=(), dtype=float32)\n",
      "tf.Tensor(1.7276932, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(fluency_score(a))\n",
    "print(fluency_score(b))\n",
    "print(fluency_score(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "72849026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024390243902439025\n",
      "0.03125\n",
      "0.023255813953488372\n"
     ]
    }
   ],
   "source": [
    "print(sentence_gleu(test_m_2.iloc[120]['correct'],a,max_len=1))\n",
    "print(sentence_gleu(test_m_2.iloc[120]['correct'],b,max_len=1))\n",
    "print(sentence_gleu(test_m_2.iloc[120]['correct'],i,max_len=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7bbb7",
   "metadata": {},
   "source": [
    "## 2. Using both the models at inference with comparison of best sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4b7b2a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i started working from the end week of july\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i started working at the end of july'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(sentence):\n",
    "    predicted_sentence_1=predict_m2(sentence)\n",
    "    f1=fluency_score(predicted_sentence_1)\n",
    "    predicted_sentence_2=predict_m1(sentence)\n",
    "    f2=fluency_score(predicted_sentence_2)\n",
    "    if f1>f2:\n",
    "        return predicted_sentence_1\n",
    "    else:\n",
    "        return predicted_sentence_2\n",
    "i=test_m_2.iloc[120]['incorrect']\n",
    "print(i)\n",
    "inference(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0984cc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500/500 [14:20<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3835768862006597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the Glue Score\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "def calculate_glue_on_df(df, predict):\n",
    "    glue_score_arr = []\n",
    "    for i in tqdm(range(500)):\n",
    "        reference = [df['correct'].iloc[i].split()]\n",
    "        pred = predict(df['incorrect'].iloc[i])\n",
    "        candidate = pred.split()\n",
    "        glue_score_arr.append(sentence_gleu(reference, candidate))\n",
    "    return np.mean(glue_score_arr)\n",
    "\n",
    "def gleu(sentence):\n",
    "  pred = predict(sentence)[0]\n",
    "  return sentence_gleu([sentence.split()], pred.split())\n",
    "\n",
    "        \n",
    "glue_m2=calculate_glue_on_df(test_m_2, inference)\n",
    "print(glue_m2)\n",
    "\n",
    "#http://nlpprogress.com/english/grammatical_error_correction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e9c8afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2m=0.3835768862006597"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c97d4e",
   "metadata": {},
   "source": [
    "## 3. Using both models in the predict function by considering the fluency score of both models outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "03b7c659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Incorrect:  i started working from the end week of july\n",
      "Predicted All Inference: i started working in the end of week in july\n",
      "Predicted Model 1 alone: i started working from the end of week in july\n",
      "Predicted Model 2 Alone: i started working at the end of july\n"
     ]
    }
   ],
   "source": [
    "def predict_all(input_sentence):\n",
    "    \n",
    "    input_sentence_2=' '.join(input_sentence.split(' ')[::-1])\n",
    "    words=[]\n",
    "    input_sentence_1=[input_sentence]\n",
    "    input_sentence_2=[input_sentence_2]\n",
    "    batch_size=1\n",
    "    \n",
    "    tokenized_sent_1=incorrect_tk.texts_to_sequences(input_sentence_1)\n",
    "    tokenized_sent_2=incorrect_tk.texts_to_sequences(input_sentence_2)\n",
    "    \n",
    "    #print(tokenized_sent)\n",
    "    padded_sent_1=tf.keras.utils.pad_sequences(tokenized_sent_1, maxlen=16,padding='post' )\n",
    "    padded_sent_2=tf.keras.utils.pad_sequences(tokenized_sent_2, maxlen=16,padding='post' )\n",
    "    \n",
    "    encoder_h_1, encoder_c_1=model_1.layers[0].initialize_states(batch_size)\n",
    "    encoder_h_2, encoder_c_2=model_2.layers[0].initialize_states(batch_size)\n",
    "    \n",
    "    encoder_output_1,encoder_h_1, encoder_c_1= model_1.layers[0](padded_sent_1, states=[encoder_h_1, encoder_c_1])\n",
    "    encoder_output_2,encoder_h_2, encoder_c_2= model_2.layers[0](padded_sent_2, states=[encoder_h_2, encoder_c_2])\n",
    "    \n",
    "    start_index=correct_tk.word_index.get('<start>')\n",
    "    end_index=correct_tk.word_index.get('<end>')\n",
    "    for i in range(16):\n",
    "        decoder_output_1, decoder_h_1, decoder_c_1, attention_weights_1, context_vector_1 = model_1.layers[1].osd(tf.convert_to_tensor([[start_index]]), encoder_output_1, encoder_h_1, encoder_c_1)\n",
    "        decoder_output_2, decoder_h_2, decoder_c_2, attention_weights_2, context_vector_2 = model_2.layers[1].osd(tf.convert_to_tensor([[start_index]]), encoder_output_2, encoder_h_2, encoder_c_2)\n",
    "        \n",
    "        output_index_1=np.argmax(decoder_output_1[0])\n",
    "        output_index_2=np.argmax(decoder_output_2[0])\n",
    "        \n",
    "        #print(output_index_1,output_index_2)\n",
    "        w_1=correct_tk.index_word[output_index_1]\n",
    "        w_2=correct_tk.index_word[output_index_2]\n",
    "        #print(w_1,w_2)\n",
    "        context=input_sentence.split(' ')\n",
    "        if i==0:\n",
    "            context[i]='[MASK]'\n",
    "            context=' '.join(context)\n",
    "        else:\n",
    "            context=' '.join(words) +' [MASK]'\n",
    "            \n",
    "        #print(w_1,context)\n",
    "        f1=p_x(w_1,context)\n",
    "        f2=p_x(w_2,context)\n",
    "        #print(w_1,w_2)\n",
    "        if f1>f2:\n",
    "            final_word=w_1\n",
    "        else:\n",
    "            final_word=w_2\n",
    "        \n",
    "        output_index=correct_tk.word_index[final_word]\n",
    "        \n",
    "        start_index=output_index\n",
    "        \n",
    "        #print(output_index)\n",
    "        encoder_h_1, encoder_c_1=decoder_h_1, decoder_c_1\n",
    "        encoder_h_2, encoder_c_2=decoder_h_2, decoder_c_1\n",
    "        if i==0:\n",
    "            words.append(final_word)\n",
    "        else:\n",
    "            if words[-1]!=correct_tk.index_word[output_index]:\n",
    "                words.append(final_word)\n",
    "\n",
    "        #print(list(tknizer_eng.word_index.keys())[output_index])\n",
    "        \n",
    "        output_index=correct_tk.word_index[final_word]\n",
    "        if output_index==end_index:\n",
    "            break;\n",
    "\n",
    "    return ' '.join(words[:-1])\n",
    "\n",
    "i=test_m_2.iloc[120]['incorrect']\n",
    "print('Original Incorrect: ',i)\n",
    "print('Predicted All Inference:',predict_all(i))\n",
    "print('Predicted Model 1 alone:',predict_m1(i))\n",
    "print('Predicted Model 2 Alone:',predict_m2(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e88f9e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.0858815, shape=(), dtype=float32)\n",
      "tf.Tensor(2.0550692, shape=(), dtype=float32)\n",
      "tf.Tensor(2.2011206, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(fluency_score(predict_all(i)))\n",
    "print(fluency_score(predict_m1(i)))\n",
    "print(fluency_score(predict_m2(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "704fe77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500/500 [17:31<00:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24515397769963335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the Glue Score\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "def calculate_glue_on_df(df, predict):\n",
    "    glue_score_arr = []\n",
    "    for i in tqdm(range(500)):\n",
    "        reference = [df['correct'].iloc[i].split()]\n",
    "        pred = predict(df['incorrect'].iloc[i])\n",
    "        candidate = pred.split()\n",
    "        glue_score_arr.append(sentence_gleu(reference, candidate))\n",
    "    return np.mean(glue_score_arr)\n",
    "\n",
    "def gleu(sentence):\n",
    "  pred = predict(sentence)[0]\n",
    "  return sentence_gleu([sentence.split()], pred.split())\n",
    "\n",
    "        \n",
    "glue_m2=calculate_glue_on_df(test_m_2, predict_all)\n",
    "print(glue_m2)\n",
    "\n",
    "#http://nlpprogress.com/english/grammatical_error_correction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "61365c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x=0.24515397769963335"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f47dda",
   "metadata": {},
   "source": [
    "## Note:\n",
    "- The GLUE Score is less for this model, however again it performs well for some type of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b90987",
   "metadata": {},
   "source": [
    "### Comparing the output from all the 3 models and providing the sentence with best fluency score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "35b2f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i started working from the end week of july\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i started working at the end of july'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference_3(sentence):\n",
    "    score=dict()\n",
    "    sentence_dict=dict()\n",
    "    sentence_dict['f1']=predict_m2(sentence)\n",
    "    score['f1']=fluency_score(sentence_dict['f1'])\n",
    "    sentence_dict['f2']=predict_m1(sentence)\n",
    "    score['f2']=fluency_score(sentence_dict['f2'])\n",
    "    sentence_dict['f3']=predict_all(sentence)\n",
    "    score['f3']=fluency_score(sentence_dict['f3'])\n",
    "    i=np.argmax(list(score.values()))\n",
    "    return sentence_dict[list(score.keys())[i]]\n",
    "i=test_m_2.iloc[120]['incorrect']\n",
    "print(i)\n",
    "inference_3(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "46fc3cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500/500 [39:19<00:00,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3364452375595764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the Glue Score\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "def calculate_glue_on_df(df, predict):\n",
    "    glue_score_arr = []\n",
    "    for i in tqdm(range(500)):\n",
    "        reference = [df['correct'].iloc[i].split()]\n",
    "        pred = predict(df['incorrect'].iloc[i])\n",
    "        candidate = pred.split()\n",
    "        glue_score_arr.append(sentence_gleu(reference, candidate))\n",
    "    return np.mean(glue_score_arr)\n",
    "\n",
    "def gleu(sentence):\n",
    "  pred = predict(sentence)[0]\n",
    "  return sentence_gleu([sentence.split()], pred.split())\n",
    "\n",
    "        \n",
    "glue_m2=calculate_glue_on_df(test_m_2, inference_3)\n",
    "print(glue_m2)\n",
    "\n",
    "#http://nlpprogress.com/english/grammatical_error_correction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "85af40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3m=0.3364452375595764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "758e3744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+---------------------+\n",
      "|                         Inference Type                         |      GLUE Score     |\n",
      "+----------------------------------------------------------------+---------------------+\n",
      "|                  Only Model 1(Left to Right)                   | 0.37295990225914544 |\n",
      "|                  Only Model 2(Right to Left)                   |  0.3876501519122926 |\n",
      "|          Best among the 2 models using Fluency Score           |  0.3835768862006597 |\n",
      "|                   For loop between 2 models                    |  0.2667352870663321 |\n",
      "| Checking the likelihood of words at inference between 2 models | 0.24515397769963335 |\n",
      "|             Comparing all the 3 inference methods              |  0.3364452375595764 |\n",
      "+----------------------------------------------------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "myTable = PrettyTable([\"Inference Type\", \"GLUE Score\"])\n",
    "myTable.add_row([\"Only Model 1(Left to Right)\", m1])\n",
    "myTable.add_row([\"Only Model 2(Right to Left)\", m2])\n",
    "myTable.add_row([\"Best among the 2 models using Fluency Score\", c2m])\n",
    "myTable.add_row([\"For loop between 2 models\", paper])\n",
    "myTable.add_row([\"Checking the likelihood of words at inference between 2 models\",p_x ])\n",
    "myTable.add_row([\"Comparing all the 3 inference methods\", c3m])\n",
    "\n",
    "print(myTable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d29138",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- An important observation is that not every model is performing good for all types of sentences.\n",
    "- And the quality of the sentences for training play an important role, because I had trained the same model on a different type of data because of which I was getting really poor results.\n",
    "- Adding more quality data like NUCLE can boost the performance of the model, as I understood that the quality of data is most important for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "dcb3de32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Correct: do not stay in the same place you need to go out to see the world\n",
      "Original Incorrect: do not stay the same place you need to go out to see the world\n",
      "________________________________________________________________________________________________________________________\n",
      "Model_1: do not stay in the same place you need to go out to watch world\n",
      "Model_2: not do not stay the place you need to go to see the sea\n",
      "As defined in Paper: do not live in the place you just go to watch them\n",
      "Best Among 2 models: not do not stay the place you need to go to see the sea\n",
      "Using likelihood at inference: do not stay in the same place you need to go out to watch world\n",
      "Best among 3: not do not stay the place you need to go to see the sea\n",
      "************************************************************************************************************************\n",
      "Original Correct: i want to improve my english writing skill especially polite english writing on lang\n",
      "Original Incorrect: my perpose in lang is to improve my english writing skill especially polite english writing\n",
      "________________________________________________________________________________________________________________________\n",
      "Model_1: my purpose in lang is to improve my english writing skills to improve writing english\n",
      "Model_2: my purpose is to writing my english writing skills in english especially to express english\n",
      "As defined in Paper: my purpose to improve my english writing skills in english writing my english skills\n",
      "Best Among 2 models: my purpose is to writing my english writing skills in english especially to express english\n",
      "Using likelihood at inference: my english writing is in my english is writing in english is your natural writing\n",
      "Best among 3: my english writing is in my english is writing in english is your natural writing\n",
      "************************************************************************************************************************\n",
      "Original Correct: these are made of cardboard\n",
      "Original Incorrect: these are made by card boad\n",
      "________________________________________________________________________________________________________________________\n",
      "Model_1: these are made by credit card\n",
      "Model_2: these are made by a card\n",
      "As defined in Paper: i received my credit card credit card\n",
      "Best Among 2 models: these are made by a card\n",
      "Using likelihood at inference: these are of india is of a study abroad countries set\n",
      "Best among 3: these are made by a card\n",
      "************************************************************************************************************************\n",
      "Original Correct: that was the key to opening the treasure chest full of golden opportunities\n",
      "Original Incorrect: that was the key to open the treasure trunk full of golden options\n",
      "________________________________________________________________________________________________________________________\n",
      "Model_1: that was the key to the key river is totally fresh for downtown\n",
      "Model_2: that was the key to open the open size of bbq rooms\n",
      "As defined in Paper: the key to the key totally is totally cleaning\n",
      "Best Among 2 models: that was the key to open the open size of bbq rooms\n",
      "Using likelihood at inference: that was the key to the key\n",
      "Best among 3: that was the key to the key\n",
      "************************************************************************************************************************\n",
      "Original Correct: i am not doing well\n",
      "Original Incorrect: i am not do well\n",
      "________________________________________________________________________________________________________________________\n",
      "Model_1: but i am not doing well\n",
      "Model_2: i am not doing well\n",
      "As defined in Paper: but i did not connect to connect them in them\n",
      "Best Among 2 models: i am not doing well\n",
      "Using likelihood at inference: i am not doing well without doing the so\n",
      "Best among 3: i am not doing well\n",
      "************************************************************************************************************************\n",
      "Original Correct: i have been there once before when i was a junior high school student\n",
      "Original Incorrect: i have been there once when i was a junior high school student\n",
      "________________________________________________________________________________________________________________________\n",
      "Model_1: i have been there once when i was a junior high school student\n",
      "Model_2: i have been there once there was a student when i was junior high school student\n",
      "As defined in Paper: i once i was there once a student i was in junior high school\n",
      "Best Among 2 models: i have been there once there was a student when i was junior high school student\n",
      "Using likelihood at inference: i have been there once when i was a junior high school student english\n",
      "Best among 3: i have been there once there was a student when i was junior high school student\n",
      "************************************************************************************************************************\n",
      "Original Correct: it was my fault but her service was very bad like a supermarket clerk\n",
      "Original Incorrect: it was my fault but her service was very bad like supermarket clerk\n",
      "________________________________________________________________________________________________________________________\n",
      "Model_1: it was my fault but the her office was such a very big surprise\n",
      "Model_2: it was her wrong but it was bad at the same case supermarket is a moment\n",
      "As defined in Paper: it was my office but was very surprised but surprised\n",
      "Best Among 2 models: it was her wrong but it was bad at the same case supermarket is a moment\n",
      "Using likelihood at inference: it is my fault but the her country was a very good study\n",
      "Best among 3: it was her wrong but it was bad at the same case supermarket is a moment\n",
      "************************************************************************************************************************\n",
      "Original Correct: therefore the way you diagnose qi plays an important role in disease identification\n",
      "Original Incorrect: therefore the way how you diagnose qi plays an important role in disease identification\n",
      "________________________________________________________________________________________________________________________\n",
      "Model_1: therefore that is why you call an important role in an critical role in an explosion\n",
      "Model_2: therefore the way some important robot how important role has an important role in\n",
      "As defined in Paper: therefore why do you think an important role in an important role in an important role\n",
      "Best Among 2 models: therefore the way some important robot how important role has an important role in\n",
      "Using likelihood at inference: therefore that is why you call ' an important ' an eye disease\n",
      "Best among 3: therefore that is why you call ' an important ' an eye disease\n",
      "************************************************************************************************************************\n",
      "Original Correct: i am chinese\n",
      "Original Incorrect: i am a chinese\n",
      "________________________________________________________________________________________________________________________\n",
      "Model_1: i am chinese\n",
      "Model_2: i am chinese\n",
      "As defined in Paper: i am chinese\n",
      "Best Among 2 models: i am chinese\n",
      "Using likelihood at inference: i am falling\n",
      "Best among 3: i am falling\n",
      "************************************************************************************************************************\n",
      "Original Correct: what do you say an elephant sounds like in your country?\n",
      "Original Incorrect: what do you say a elephant voice in your country?\n",
      "________________________________________________________________________________________________________________________\n",
      "Model_1: what do you say a voice in your country\n",
      "Model_2: what do you say a habit of your country is thinking\n",
      "As defined in Paper: what do you say a voice in your country\n",
      "Best Among 2 models: what do you say a voice in your country\n",
      "Using likelihood at inference: what is you is the voice is in your country than a abroad than you\n",
      "Best among 3: what do you say a voice in your country\n",
      "************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    j=random.randint(0,len(test_m_1))\n",
    "    sentence=test_m_2.iloc[j]['incorrect']\n",
    "    print('Original Correct:',test_m_2.iloc[j]['correct'])\n",
    "    print('Original Incorrect:',sentence)\n",
    "    print('_'*120)\n",
    "    print('Model_1:',predict_m1(sentence))\n",
    "    print('Model_2:',predict_m2(sentence))\n",
    "    print('As defined in Paper:',inference_1(sentence))\n",
    "    print('Best Among 2 models:',inference(sentence))\n",
    "    print('Using likelihood at inference:',predict_all(sentence))\n",
    "    print('Best among 3:',inference_3(sentence)) \n",
    "    print('*'*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc3913",
   "metadata": {},
   "source": [
    "**Final Conclusion**\n",
    "\n",
    "As you may see, no single model is performing best but some models perform well for some sentences and some for other, hence if I would have to deploy a model, I would prefer the \"Best of 2 model\" because it has a score equal to Model_1 & Model_2 and also has the advantage of both the models.\n",
    "\n",
    "The approach mentioned by the paper, has failed in my approach because as the error in model_1 increases, it further amplifies in the next iteration, hence better quality of data can solve this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
